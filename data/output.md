The main benchmark table appears on pages 3 and 6.

Here's the benchmark data formatted as requested:

| Benchmark | Claude 3 Opus | Claude 3 Sonnet | Claude 3 Haiku | GPT-4 | GPT-3.5 | Gemini 1.0 Ultra | Gemini 1.5 Pro | Gemini 1.0 Pro | Source/Reference |
|-----------|---------------|-----------------|----------------|--------|----------|------------------|-----------------|----------------|-----------------|
| MMLU (5-shot) | 86.8% | 79.0% | 75.2% | 86.4% | 70.0% | 83.7% | 81.9% | 71.8% | "Measuring Massive Multitask Language Understanding" |
| MATH (4-shot) | 61% | 40.5% | 40.9% | 52.9% | 34.1% | 53.2% | 58.5% | 32.6% | "Measuring Mathematical Problem Solving With the MATH Dataset" |
| GSM8K (0-shot CoT) | 95.0% | 92.3% | 88.9% | 92.0% | 57.1% | 94.4% | 91.7% | 86.5% | "Training Verifiers to Solve Math Word Problems" |
| HumanEval (0-shot) | 84.9% | 73.0% | 75.9% | 67.0% | 48.1% | 74.4% | 71.9% | 67.7% | "Evaluating Large Language Models Trained on Code" |
| GPQA Diamond (0-shot CoT) | 50.4% | 40.4% | 33.3% | 35.7% | 28.1% | – | – | – | https://arxiv.org/abs/2311.12022 |
| MGSM (0-shot) | 90.7% | 83.5% | 75.1% | 74.5% | – | 79.0% | 88.7% | 63.5% | "Language Models are Multilingual Chain-of-Thought Reasoners" |
| DROP (F1 Score) | 83.1 | 78.9 | 78.4 | 80.9 | 64.1 | 82.4 | 78.9 | 74.1 | "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs" |
| BIG-Bench-Hard (3-shot CoT) | 86.8% | 82.9% | 73.7% | 83.1% | 66.6% | 83.6% | 84.0% | 75.0% | "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models" |
| ARC-Challenge (25-shot) | 96.4% | 93.2% | 89.2% | 96.3% | 85.2% | – | – | – | "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge" |
| HellaSwag (10-shot) | 95.4% | 89.0% | 85.9% | 95.3% | 85.5% | 87.8% | 92.5% | 84.7% | "HellaSwag: Can a Machine Really Finish Your Sentence?" |
| PubMedQA (5-shot) | 75.8% | 78.3% | 76.0% | 74.4% | 60.2% | – | – | – | "PubMedQA: A Dataset for Biomedical Research Question Answering" |
| WinoGrande (5-shot) | 88.5% | 75.1% | 74.2% | 87.5% | – | – | – | – | "WinoGrande: An Adversarial Winograd Schema Challenge at Scale" |
| RACE-H (5-shot) | 92.9% | 88.8% | 87.0% | – | – | – | – | – | "RACE: Large-scale ReAding Comprehension Dataset From Examinations" |
| APPS (0-shot) | 70.2% | 55.9% | 54.8% | – | – | – | – | – | "Measuring Coding Challenge Competence With APPS" |
| MBPP (Pass@1) | 86.4% | 79.4% | 80.4% | – | – | – | – | – | "Program Synthesis with Large Language Models" |